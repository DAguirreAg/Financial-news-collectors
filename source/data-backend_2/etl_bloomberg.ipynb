{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b05d489c-c366-4f58-956d-baa33b55e7c3",
   "metadata": {},
   "source": [
    "# Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d45dbf5-ef7f-4923-b33c-ae6b54be44aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "from bs4 import BeautifulSoup, element\n",
    "import re\n",
    "from config import Config\n",
    "\n",
    "import utils\n",
    "import sql_utils\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e589c4-b34b-4a50-ae21-62f6497ef014",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d9b743-9c71-4676-a8d3-71bb7959d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(headline: element.Tag):\n",
    "        \n",
    "    # Title\n",
    "    try:\n",
    "        #title = headline.find(\"a\", {\"class\": \"nw-o-link-split__anchor\"}).text\n",
    "        title = headline.find(\"a\").text\n",
    "    \n",
    "    except:\n",
    "        title = None\n",
    "        \n",
    "    finally:\n",
    "        if title:\n",
    "            title = title.strip()\n",
    "    \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "864926b1-687a-45f6-98f5-216c1481cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(headline: element.Tag):\n",
    "    \n",
    "    # Url\n",
    "    try:        \n",
    "        href = headline.find(\"a\")['href']\n",
    "\n",
    "    except:\n",
    "        href = None\n",
    "        \n",
    "    finally:\n",
    "        if href:\n",
    "            url = 'https://www.bbc.com' + href\n",
    "        else:\n",
    "            url = href\n",
    "    \n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79f2a999-c6bc-4c9f-a1e7-2c29c76d8376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_publish_date(headline: element.Tag):\n",
    "    \n",
    "    # Publish date\n",
    "    publish_date = None\n",
    "    \n",
    "    try:\n",
    "        # Get publish datetime\n",
    "        #publish_date = headline.find('time', {'class': 'date'})['datetime']\n",
    "        publish_date = headline.find('span', {'class': 'e16en2lz0'}).text\n",
    "        \n",
    "        if len(publish_date) > 20: # Sometimes the wrong tag is picked\n",
    "            publish_date = None\n",
    "\n",
    "    except IndexError as e:\n",
    "        #print(e)\n",
    "        pass\n",
    "    \n",
    "    except ValueError as e:\n",
    "        #print(e)\n",
    "        pass\n",
    "    \n",
    "    except TypeError as e:\n",
    "        #print(e)\n",
    "        pass\n",
    "\n",
    "    return publish_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "097f1476-0d14-4d02-9e08-4d7b22b3ef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tickers(headline: element.Tag):\n",
    "    \n",
    "    # Tickers\n",
    "    tickers = []\n",
    "\n",
    "    return tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f4cfb67-93a5-4ec1-b732-bd8a545ed770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_headline_data(headline: element.Tag):\n",
    "    \n",
    "    # Extract data\n",
    "    extract = {    \n",
    "        'title': get_title(headline),\n",
    "        'url': get_url(headline),\n",
    "        'publish_date': get_publish_date(headline),\n",
    "        'tickers': get_tickers(headline),\n",
    "        'countries': utils.get_countries(get_title(headline))\n",
    "    }\n",
    "    \n",
    "    return extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae1d387e-274b-4f87-bdbe-93d9468caba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headlines(soup: BeautifulSoup):\n",
    "    # Get headlines\n",
    "    #headlines = soup.find_all('div', {'class': 'gel-layout__item'})\n",
    "    headlines = soup.find_all('div', {'class': 'ssrcss-tq7xfh-PromoContent exn3ah99'})\n",
    "\n",
    "    # Extract info from headlines\n",
    "    news = []\n",
    "    for headline in headlines:\n",
    "        extract = extract_headline_data(headline)\n",
    "        news.append(extract)\n",
    "\n",
    "    return news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa114411-04a5-465b-b24f-cc9f9be741e1",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5048b8ea-3bcf-4e47-99e2-6b5c7b37a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(filepath: str)->dict:\n",
    "    \n",
    "    with open(filepath, 'r') as j:\n",
    "        file_content = json.loads(j.read())\n",
    "        \n",
    "    return file_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e6539ba-b0af-4085-8c21-3f791edbf41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(file_content: str, filepath: str)->dict:\n",
    "    \n",
    "    # Extract    \n",
    "    html = file_content['html']\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Get headlines\n",
    "    news = get_headlines(soup)\n",
    "\n",
    "    # Add metadata\n",
    "    ref_filename = filepath.split('/')[-1]\n",
    "    downloaded_datetime = file_content['downloaded_datetime']\n",
    "    \n",
    "    for i in range(len(news)):\n",
    "        news[i]['news_outlet'] = re.search('https?://([A-Za-z_0-9.-]+).*', file_content['url']).group(1)\n",
    "        news[i]['ref_filename'] = ref_filename\n",
    "        news[i]['downloaded_datetime'] = downloaded_datetime\n",
    "        \n",
    "\n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f1ce9bb-84ae-45a3-9185-3cb12fe73507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_extract(data: List[Dict], schema: dict)->bool:\n",
    "    \n",
    "    for row in data:\n",
    "    \n",
    "        # Check if all columns appear\n",
    "        for c in schema.keys():\n",
    "            if c not in row:\n",
    "                print(c)\n",
    "                return False\n",
    "\n",
    "        # Check for nonnullability\n",
    "        title = row['title']\n",
    "        url = row['url']\n",
    "        publish_date = row['publish_date']\n",
    "        if title is None or title == '':\n",
    "            return False\n",
    "\n",
    "        if url is None or url == '':\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f009bb6-44b6-4ea6-9117-7059498664ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(data: dict):\n",
    "    sql_utils.insert_data_into_db_news_table(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "660b9549-9ec5-445f-bf1f-12a7947f4a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Get files to process\n",
    "    etl_consumption_path = Config.NEWS_SETTINGS['BBC']['DOWNLOAD_PATH'] + str(datetime.datetime.now().date()).replace('-', '')\n",
    "    filepaths = glob.glob(os.path.join(etl_consumption_path, \"*.json\"))\n",
    "\n",
    "    # Reupload the missing files\n",
    "    for filepath in filepaths:\n",
    "        print(f'ETL: {filepath}')\n",
    "\n",
    "        # Extract file\n",
    "        file_content = extract(filepath)\n",
    "            \n",
    "        # Transform\n",
    "        data = transform(file_content, filepath)\n",
    "        \n",
    "        # Validate\n",
    "        is_valid = validate_extract(data, Config.STAGING_SCHEMA)\n",
    "        \n",
    "        # Load file\n",
    "        if is_valid:\n",
    "            load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9db555e-09cf-45ca-a194-ddb4c4f7afe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL: downloads/bbc/20231106/20231106_185003_219852.json\n",
      "ETL: downloads/bbc/20231106/20231106_184939_580158.json\n",
      "ETL: downloads/bbc/20231106/20231106_184927_648007.json\n",
      "ETL: downloads/bbc/20231106/20231106_184951_725009.json\n",
      "ETL: downloads/bbc/20231106/20231106_184945_706346.json\n",
      "ETL: downloads/bbc/20231106/20231106_184957_434518.json\n",
      "ETL: downloads/bbc/20231106/20231106_184933_321954.json\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
